{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import torch.nn.functional as F \n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np, os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import itertools # For plotting CM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_TYPE = \"pressure\" \n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32 # 500 images is small, a 32 batch size is fine\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "EPOCHS = 15      # 15-20 is plenty for fine-tuning just the head\n",
        "PATIENCE = 4     # Stop after 4 epochs with no improvement\n",
        "K_FOLDS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data Augmentation (Slightly lighter) ---\n",
        "tfm_train = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(IMG_SIZE), # Good choice for this data\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.RandomAffine(degrees=5), # Small rotation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "tfm_val = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dataset: pressure\n",
            "Train dir: c:\\Users\\Sai\\Desktop\\Tyre health POC\\tyre-health-poc\\data\\raw\\pressure\\train\n",
            "Val dir:   c:\\Users\\Sai\\Desktop\\Tyre health POC\\tyre-health-poc\\data\\raw\\pressure\\val\n",
            "Test dir (Final Holdout): c:\\Users\\Sai\\Desktop\\Tyre health POC\\tyre-health-poc\\data\\raw\\pressure\\test\n",
            "Classes: ['flat', 'full']\n",
            "Total images for K-Fold (Train+Val): 540\n",
            "Total test images (hold-out): 60\n",
            "Combined training counts: {'flat': np.int64(270), 'full': np.int64(270)}\n",
            "Dataset appears balanced. No class weights needed.\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# --- Load Datasets & Combine for K-Fold ---\n",
        "PROJECT_ROOT = Path.cwd().parent\n",
        "DATA_ROOT = PROJECT_ROOT / \"data\" / \"raw\" / DATASET_TYPE\n",
        "TRAIN_DIR = DATA_ROOT / \"train\"\n",
        "VAL_DIR   = DATA_ROOT / \"val\" # <-- We'll load this too\n",
        "TEST_DIR  = DATA_ROOT / \"test\"\n",
        "\n",
        "print(f\"Using dataset: {DATASET_TYPE}\")\n",
        "print(f\"Train dir: {TRAIN_DIR}\")\n",
        "print(f\"Val dir:   {VAL_DIR}\")\n",
        "print(f\"Test dir (Final Holdout): {TEST_DIR}\")\n",
        "\n",
        "# 1. Load both train and val datasets initially\n",
        "#    Use the TRAINING transform for BOTH initially, as they will be used for training in K-Fold\n",
        "train_ds_part = datasets.ImageFolder(TRAIN_DIR, transform=tfm_train)\n",
        "val_ds_part   = datasets.ImageFolder(VAL_DIR,   transform=tfm_train) # Use train transform here too\n",
        "\n",
        "# 2. Combine them into one dataset for K-Fold\n",
        "combined_train_ds = ConcatDataset([train_ds_part, val_ds_part])\n",
        "num_classes = len(train_ds_part.classes) # Get classes from one part\n",
        "\n",
        "# Keep the test set separate with its own transform\n",
        "test_ds = datasets.ImageFolder(TEST_DIR, transform=tfm_val)\n",
        "\n",
        "print(f\"Classes: {train_ds_part.classes}\")\n",
        "print(f\"Total images for K-Fold (Train+Val): {len(combined_train_ds)}\")\n",
        "print(f\"Total test images (hold-out): {len(test_ds)}\")\n",
        "\n",
        "# --- Check balance (on the combined set) ---\n",
        "# Need to get targets from both parts\n",
        "combined_targets = np.concatenate([train_ds_part.targets, val_ds_part.targets])\n",
        "counts = np.bincount(combined_targets)\n",
        "print(f\"Combined training counts: {dict(zip(train_ds_part.classes, counts))}\")\n",
        "print(\"Dataset appears balanced. No class weights needed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    \"\"\"Helper function to create a fresh, frozen ResNet-18 model.\"\"\"\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    \n",
        "    # 1. Freeze all layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    # 2. Replace the head (this new layer will be trainable by default)\n",
        "    in_feats = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_feats, num_classes)\n",
        "    \n",
        "    model = model.to(device)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, loader, crit, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = crit(logits, y)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred==y).sum().item()\n",
        "            total += y.size(0)\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "            y_pred.extend(pred.cpu().numpy())\n",
        "    acc = correct/total\n",
        "    return acc, loss_sum/total, np.array(y_true), np.array(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLflow Experiment: Tyre_Pressure_ResNet18\n",
            "--- Starting 5-Fold Cross Validation ---\n",
            "\n",
            "==================================================\n",
            "FOLD 1/5\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1/15: train_loss=0.7326 val_loss=1.0906 val_acc=0.4352\n",
            "  🎉 New best! (acc=0.4352)\n",
            "Epoch  2/15: train_loss=0.6492 val_loss=1.2827 val_acc=0.4352\n",
            "Epoch  3/15: train_loss=0.5753 val_loss=0.6945 val_acc=0.5370\n",
            "  🎉 New best! (acc=0.5370)\n",
            "Epoch  4/15: train_loss=0.5072 val_loss=0.5233 val_acc=0.7963\n",
            "  🎉 New best! (acc=0.7963)\n",
            "Epoch  5/15: train_loss=0.4799 val_loss=0.5239 val_acc=0.7130\n",
            "Epoch  6/15: train_loss=0.4658 val_loss=0.4393 val_acc=0.8333\n",
            "  🎉 New best! (acc=0.8333)\n",
            "Epoch  7/15: train_loss=0.4207 val_loss=0.4221 val_acc=0.8611\n",
            "  🎉 New best! (acc=0.8611)\n",
            "Epoch  8/15: train_loss=0.4345 val_loss=0.4004 val_acc=0.8519\n",
            "Epoch  9/15: train_loss=0.3905 val_loss=0.4075 val_acc=0.8241\n",
            "Epoch 10/15: train_loss=0.3750 val_loss=0.3856 val_acc=0.8333\n",
            "Epoch 11/15: train_loss=0.3514 val_loss=0.3659 val_acc=0.8704\n",
            "  🎉 New best! (acc=0.8704)\n",
            "Epoch 12/15: train_loss=0.3287 val_loss=0.3479 val_acc=0.8426\n",
            "Epoch 13/15: train_loss=0.3489 val_loss=0.3350 val_acc=0.8704\n",
            "Epoch 14/15: train_loss=0.3113 val_loss=0.3329 val_acc=0.8796\n",
            "  🎉 New best! (acc=0.8796)\n",
            "Epoch 15/15: train_loss=0.3163 val_loss=0.3529 val_acc=0.8519\n",
            "Fold 1 completed. Best accuracy: 0.8796\n",
            "\n",
            "==================================================\n",
            "FOLD 2/5\n",
            "==================================================\n",
            "Epoch  1/15: train_loss=0.7650 val_loss=0.8062 val_acc=0.5093\n",
            "  🎉 New best! (acc=0.5093)\n",
            "Epoch  2/15: train_loss=0.6402 val_loss=0.6194 val_acc=0.5926\n",
            "  🎉 New best! (acc=0.5926)\n",
            "Epoch  3/15: train_loss=0.5486 val_loss=0.5004 val_acc=0.8611\n",
            "  🎉 New best! (acc=0.8611)\n",
            "Epoch  4/15: train_loss=0.5144 val_loss=0.4756 val_acc=0.8148\n",
            "Epoch  5/15: train_loss=0.4710 val_loss=0.5968 val_acc=0.6204\n",
            "Epoch  6/15: train_loss=0.4831 val_loss=0.4673 val_acc=0.7593\n",
            "Epoch  7/15: train_loss=0.3830 val_loss=0.3906 val_acc=0.8796\n",
            "  🎉 New best! (acc=0.8796)\n",
            "Epoch  8/15: train_loss=0.3952 val_loss=0.3686 val_acc=0.8981\n",
            "  🎉 New best! (acc=0.8981)\n",
            "Epoch  9/15: train_loss=0.3663 val_loss=0.3473 val_acc=0.8981\n",
            "Epoch 10/15: train_loss=0.3705 val_loss=0.3981 val_acc=0.7963\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m     train_loss_sum += loss.item() * y.size(\u001b[32m0\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m acc, vloss, y_true, y_pred = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m train_loss = train_loss_sum / \u001b[38;5;28mlen\u001b[39m(train_subset)\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, loader, crit, device)\u001b[39m\n\u001b[32m      4\u001b[39m total, correct, loss_sum = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0.0\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1437\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1434\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1435\u001b[39m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[32m   1436\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._persistent_workers:\n\u001b[32m-> \u001b[39m\u001b[32m1437\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1438\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1440\u001b[39m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[32m   1441\u001b[39m \n\u001b[32m   1442\u001b[39m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1568\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1563\u001b[39m         \u001b[38;5;28mself\u001b[39m._mark_worker_as_unavailable(worker_id, shutdown=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._workers:\n\u001b[32m   1565\u001b[39m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[32m   1566\u001b[39m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[32m   1567\u001b[39m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1568\u001b[39m     \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_queues:\n\u001b[32m   1570\u001b[39m     q.cancel_join_thread()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py:149\u001b[39m, in \u001b[36mBaseProcess.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parent_pid == os.getpid(), \u001b[33m'\u001b[39m\u001b[33mcan only join a child process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mcan only join a started process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_popen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     _children.discard(\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:112\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    110\u001b[39m     msecs = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m + \u001b[32m0.5\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m res = _winapi.WaitForSingleObject(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m._handle), msecs)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res == _winapi.WAIT_OBJECT_0:\n\u001b[32m    114\u001b[39m     code = _winapi.GetExitCodeProcess(\u001b[38;5;28mself\u001b[39m._handle)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# --- K-FOLD CROSS-VALIDATION LOOP ---\n",
        "\n",
        "experiment_name = \"Tyre_Pressure_ResNet18\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "print(f\"MLflow Experiment: {experiment_name}\")\n",
        "\n",
        "# Use ONLY train_ds for cross-validation\n",
        "kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "cv_scores = []\n",
        "best_models_list = [] # Store the state dict of the best model from each fold\n",
        "\n",
        "print(f\"--- Starting {K_FOLDS}-Fold Cross Validation ---\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(combined_train_ds)):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"FOLD {fold + 1}/{K_FOLDS}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Create fold subsets\n",
        "    train_subset = Subset(combined_train_ds, train_idx)\n",
        "    val_subset   = Subset(combined_train_ds, val_idx)\n",
        "    \n",
        "    # Apply the correct transforms (Subset doesn't copy them)\n",
        "    # We must clone the dataset and set the transform\n",
        "    train_subset.dataset = combined_train_ds\n",
        "    temp_val_ds_ref = ConcatDataset([\n",
        "        datasets.ImageFolder(TRAIN_DIR, transform=tfm_val),\n",
        "        datasets.ImageFolder(VAL_DIR, transform=tfm_val)\n",
        "    ])\n",
        "    val_subset.dataset = temp_val_ds_ref\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_subset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    \n",
        "    # --- Reset model and optimizer for each fold ---\n",
        "    model = create_model()\n",
        "    \n",
        "    # We are ONLY training the head\n",
        "    opt = optim.AdamW(model.fc.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    \n",
        "    # Loss function (no weights needed for balanced data)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    \n",
        "    run_name = f\"ResNet18_Fold{fold+1}\"\n",
        "    with mlflow.start_run(run_name=run_name):\n",
        "        mlflow.log_param(\"model\", \"resnet18_frozen_head\")\n",
        "        mlflow.log_param(\"fold\", fold + 1)\n",
        "        mlflow.log_param(\"epochs\", EPOCHS)\n",
        "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
        "        mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n",
        "        mlflow.log_param(\"weight_decay\", WEIGHT_DECAY)\n",
        "        \n",
        "        for epoch in range(1, EPOCHS + 1):\n",
        "            # Training\n",
        "            model.train()\n",
        "            train_loss_sum = 0.0\n",
        "            for x, y in train_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                opt.zero_grad()\n",
        "                loss = crit(model(x), y)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                train_loss_sum += loss.item() * y.size(0)\n",
        "            \n",
        "            # Validation\n",
        "            acc, vloss, y_true, y_pred = evaluate(model, val_loader, crit, device)\n",
        "            train_loss = train_loss_sum / len(train_subset)\n",
        "            \n",
        "            print(f\"Epoch {epoch:2d}/{EPOCHS}: train_loss={train_loss:.4f} val_loss={vloss:.4f} val_acc={acc:.4f}\")\n",
        "            \n",
        "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"val_loss\", vloss, step=epoch)\n",
        "            mlflow.log_metric(\"val_acc\", acc, step=epoch)\n",
        "            \n",
        "            scheduler.step(acc)\n",
        "            \n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                patience_counter = 0\n",
        "                print(f\"  🎉 New best! (acc={acc:.4f})\")\n",
        "                best_models_list.append({\n",
        "                    'fold': fold + 1,\n",
        "                    'model_state': model.state_dict().copy(),\n",
        "                    'acc': acc\n",
        "                })\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= PATIENCE:\n",
        "                    print(f\"  ⏹️  Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "        \n",
        "        cv_scores.append(best_acc)\n",
        "        mlflow.log_metric(\"best_val_acc\", best_acc)\n",
        "        print(f\"Fold {fold + 1} completed. Best accuracy: {best_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"CROSS-VALIDATION RESULTS\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Individual fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "print(f\"Mean CV Score: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
        "print(f\"Best single fold: {max(cv_scores):.4f}\")\n",
        "\n",
        "# Find best model across all folds\n",
        "best_overall_model_info = max(best_models_list, key=lambda x: x['acc'])\n",
        "print(f\"Best model: Fold {best_overall_model_info['fold']} with accuracy {best_overall_model_info['acc']:.4f}\")\n",
        "\n",
        "# Load the best model for final evaluation\n",
        "model = create_model()\n",
        "model.load_state_dict(best_overall_model_info['model_state'])\n",
        "print(f\"\\n✅ Cross-validation complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- FINAL EVALUATION ON HELD-OUT TEST SET ---\n",
        "print(\"--- Final Evaluation on Test Set ---\")\n",
        "\n",
        "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Evaluate best model on test set\n",
        "test_acc, test_loss, y_true_test, y_pred_test = evaluate(model, test_dl, crit, device)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_true_test, y_pred_test))\n",
        "print(\"\\nTest Classification Report:\\n\", classification_report(y_true_test, y_pred_test, target_names=test_ds.classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = f\"best_{DATASET_TYPE}_resnet18.pt\"\n",
        "torch.save({\n",
        "    \"model\": model.state_dict(), \n",
        "    \"classes\": test_ds.classes,\n",
        "    \"cv_scores\": cv_scores,\n",
        "    \"test_acc\": test_acc\n",
        "}, save_path)\n",
        "print(f\"\\nModel saved as: {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_for_roc(model, loader, device):\n",
        "    \"\"\"Generates probabilities for the ROC curve.\"\"\"\n",
        "    model.eval()\n",
        "    y_true_all, y_proba_all = [], []\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            probas = F.softmax(logits, dim=1)\n",
        "            y_true_all.extend(y.cpu().numpy())\n",
        "            y_proba_all.extend(probas[:, 1].cpu().numpy()) # Proba for class '1'\n",
        "    return np.array(y_true_all), np.array(y_proba_all)\n",
        "\n",
        "print(\"\\nGenerating ROC-AUC plot on test set...\")\n",
        "y_true_roc, y_proba_roc = evaluate_for_roc(model, test_dl, device)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true_roc, y_proba_roc)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(f\"Test Set AUC Score: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
        "         label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title(f'ROC Curve - {DATASET_TYPE} (Test Set)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "\n",
        "plot_filename = f\"roc_auc_plot_{DATASET_TYPE}_resnet18.png\"\n",
        "plt.savefig(plot_filename)\n",
        "print(f\"Saved ROC plot to {plot_filename}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Cross-Validation Results: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
        "print(f\"Test Set Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Set AUC: {roc_auc:.4f}\")\n",
        "print(f\"Best Model: Fold {best_overall_model_info['fold']} (CV acc: {best_overall_model_info['acc']:.4f})\")\n",
        "print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
